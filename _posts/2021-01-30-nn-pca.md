---
layout: post
title: Fast and slow dynamics computing PCA
tags: [neuronal dynamics, machine learning]
---

The brain can be viewed as a computing or information processing machine where the relevant information is extracted from the sensory inputs. Similarly to our computers, the computations can be described as a sequence of instructions with algorithms. However, the brain is also very different from our computers and others prefer to view it as a dynamical system with continuous time dynamics at multiple temporal and spatial scales. How can we combine these two points of view ?

In this blog post, I will try to provide some intuition about how the dynamics of a neural network can compute an algorithm. As an example I will consider the Principal Component Analysis (PCA) method, which is a classical method for dimensionality reduction. The idea is to project high-dimensional data onto a smaller subspace in a way that captures as much as possible the relevant aspects of the data, and we will see what does that mean more precisely. After reviewing the basics of PCA, we will use a formulation of PCA as an optimization problem to build a continuous time dynamical system that computes this algorithm. In particular, we will see the importance of having two separate time scales for the dynamics. Finally, we will see how it performs on Gaussian data and on the MNIST dataset.

<!--Two views on the brain. One is closer to artificial intelligence and sees the brain as a computing machine. Algorithms are a list of instructions that are executed one at a time.-->

<!--The other sees it as a dynamical systems and study neuronal dynamics. Dynamical systems are described by a differential equation giving the time evolution of the state of the system. So here the evolution is specified in continuous time instead of the discrete time of algorithms.-->



<!--Quite general framework that allows us to build a dynamical system whose state evolves to minimize some objective or energy function.-->



## PCA
#### Basics

This section covers the basics of PCA and can be skipped for those who are already familiar with it. Let's consider $$m$$ data points $$\mathbf{x}^{(1)}, ..., \mathbf{x}^{(m)}$$ in $$\mathbb{R}^n$$ drawn independently and identically (i.d.d.) from a distribution $$p_{data}$$, which is often called the data generating distribution. The PCA method assumes that the relevant directions are those along which the data vary the most, while those along which the data vary the least are considered as noise. It is important to note that this is an assumption and it will be satisfied or not depending on what your data represent and what you want to do with it. We can visualize more concretely what is a direction of maximal variance with a small example in $$\mathbb{R}^2$$. On the figure below is shown some data points sampled from a Gaussian distribution and we can see that there is a direction along which the data vary the most, depicted by the solid line segment. 

<img src="{{ site.baseurl }}/assets/img/nn_pca/data.png" class="center">

Let's consider the general case. The data $$\mathbf{x}^{(1)}, ..., \mathbf{x}^{(m)}$$ are assumed to be centered and let's place them in a matrix $$X$$ such that the $$i^{\text{th}}$$ row is given by $$\mathbf{x}^{(i)}$$. The goal of PCA is to find a subspace of dimension $$r$$ that captures as much variance as possible and to find an uncorrelated basis of this subspace. We will need the notion of the covariance matrix, so here is a short remark for those who are not familiar with it.

> **Remark:** *The covariance of two random variables $$x_1$$ and $$x_2$$ is defined as $$\text{cov}(x_1, x_2) = \mathbb{E}[(x_1 - \mathbb{E}[x_1])(x_2 - \mathbb{E}[x_2])]$$, which is equal to $$\mathbb{E}[x_1x_2]$$ if the variables have been centered beforehand so that they have zero mean. The entry at the $$i^{\text{th}}$$ row, $$j^{\text{th}}$$ column of the covariance matrix of a vector of random variables $$\mathbf{x} = (x_1, ..., x_n)$$ is given by $$\text{cov}(x_i, x_j)$$. The expectation over a random variable can be approximated by an average over data points sampled i.d.d. from the distribution of the random variable. Thus, for centered data, $$\text{cov}(x_i, x_j) \approx \frac{1}{m}\sum_{k=1}^m x_i^{(k)}x_j^{(k)}$$ and the covariance matrix can be approximated by $$\frac{1}{m}X^TX$$.  I will frequently jump from random variables to data samples to make my explanations more concise.*

Let's start by computing the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) $$X = U\Sigma V^T$$ and use it to define the following change of variable

$$
\mathbf{z} = V^T\mathbf{x}
$$

This change of variable can be applied on each example $$\mathbf{x}^{(i)}$$ in $$X$$ to obtain a matrix $$Z=XV$$. Then using the fact that $$X^TX = V\Sigma^2 V^T$$, the covariance marix of $$Z$$ is given by

$$
\dfrac{1}{n}Z^TZ = \frac{1}{n}\Sigma^2
$$

Since $$\Sigma$$ is diagonal, the covariance matrix of $$\mathbf{z}$$ is diagonal. So we have $$n$$ uncorrelated random variables $$z_i$$ with respective variance given by the diagonal elements of $$\Sigma^2$$, i.e. $$\text{var}(z_i) = \sigma_i^2$$. From a goemetrical point of view, the variable $$z_i$$ gives the component of the vector $$\mathbf{x}$$ projected onto the basis vector $$\mathbf{v}_i$$, i.e. the $$i^{\text{th}}$$ column of $$V^T$$. Hence, the direction of maximal variance is given by the vector $$\mathbf{v}_1$$ associated with the maximal singular value (the singular values are ordered in decreasing order) and the corresponding variance is $$\text{var}(z_1) = \sigma_1^2$$. More generally, the subspace spanned by the $$r$$ first basis vectors $$\mathbf{v}_1, ..., \mathbf{v}_r$$ associated with the $$r$$ largest singular values $$\sigma_1, ..., \sigma_r$$ captures as much variance of the data as possible.

<!--Approximate it by a low rank matrix $$W$$-->
<!--$$
\dfrac{1}{n}X^TX \approx \sigma_1^2 v_1v_1^T + ... + \sigma_r^2v_rv_r^T
$$-->



### Optimization formulation

To build a dynamical system computing the solution obtained above, it will be useful to express this solution as the solution of an optimization problem.

Consider the vector $$z$$ directly as r-dimensional and $$W$$ contains the $$r$$ first columns of $$V$$.



PCA captures the directions of maximal variance of the data. The coordinates along these directions are denoted by $$z$$. The reconstruction error for a given data point $$x$$ is given by $$ \lvert\lvert Wz-x\lvert\lvert^2 $$. 

$$ \min_{\mathbf{z}} \frac{1}{2}||W\mathbf{z}-\mathbf{x}||^2 =: R_\mathbf{x}(\mathbf{z})$$


The solution can be computed analytically using the SVD decomposition of $$X$$ 


$$
\nabla_\mathbf{z} R_\mathbf{x}(\mathbf{z}) = W^TW\mathbf{z} - W^T \mathbf{x} = 0
$$



The solution is $$z=(W^TW)^{-1}W^Tx$$ which is a well-known formula for the [orthogonal projection](https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Formulas) of a vector.



The weights $$W$$ minimizes the expectation of reconstruction error under the data generating distribution. 


$$
\min_{W} \frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_{data}}[||W\mathbf{z}-\mathbf{x}||^2] =: R_{\mathbf{z}}(W)
$$

$$
\dfrac{\partial R_{\mathbf{z}}}{\partial W_{ij}}(W) = (\hat{x}_k-x_k)z_j
$$


The matrix $$W$$ can also be computed using the SVD decomposition of the matrix $$X$$ containing the data points in its rows.





## Dynamical Equations

### Fast dynamics

Let's denote by $$\mathbf{\hat{x}}$$ the activity of the neurons computing $$W\mathbf{z}$$. Their dynamics is described by the following equation
$$
\tau \dfrac{d\mathbf{\hat{x}}}{dt} = -\mathbf{\hat{x}} + W\mathbf{z}
$$
Linear rate model.
The equilibrium solution for which $$\dfrac{d\mathbf{\hat{x}}}{dt}=0$$ is $$ \mathbf{\hat{x}} = W\mathbf{z}$$, and the rate of convergence towards this equilibrium is controlled by the time constant $$\tau$$. To obtain the dynamical equations of $$\mathbf{z}$$ and the weights $$W$$, 

> **Theorem:** *If a dynamical system can be written as $$ \frac{d\mathbf{x}}{dt} = - \nabla V(\mathbf{x})$$  then*
> 
> $$
>\frac{dV}{dt}(\mathbf{x}(t)) = \nabla V(\mathbf{x}(t)) \cdot \frac{d\mathbf{x}}{dt}(t) = -||\nabla V(\mathbf{x}(t))||^2 \leq 0
> $$
> 
> *and $$\frac{dV}{dt} = 0$$ if and only if $$\nabla V = 0$$. The function $$V$$ can only decrease on the trajectories of the system and the state of the system converges towards a minimum of $$V$$ (assuming $$V$$ is convex). This is the equivalent of the gradient descent algorithm but in continuous time.*




Knowing this, we can construct dynamical equations from the objective function that we would like to minimize.


$$
\tau \dfrac{dz}{dt} = W^T(x-\hat{x})
$$
### Slow dynamics
$$
\tau_W \dfrac{dW}{dt} = (x-\hat{x})z^T
$$

<!--Two--> different timescales. 

The slow dynamics of the weights make them sensitive to the data distribution instead of the details of individual examples.

## Simulations

<img src="{{ site.baseurl }}/assets/img/nn_pca/pca_anim.gif" class="center">



## Conclusion

Rao, predictive coding. Meta RL.



<!--Material: MOOC Coursera-->

### <!--TODO-->

- [x] <!--PCA on MNIST (only 0 and 1 digits so that can be separated with PCA), animation with the projection on the two first principal components (show them in light orange and display the normalized column vectors of W projected onto that plane)-->
- [ ] <!--Display the animation in the reference frame of the 2 first principal components and  in the reference frame represented by the learned weights of the network-->
- [ ] <!--Add a temporal filtering to z-->
- [ ] <!--Nonnegative PCA ?-->



