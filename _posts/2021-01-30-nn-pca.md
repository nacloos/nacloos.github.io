---
layout: post
title: Fast and slow dynamics performing PCA
---

Rao, predictive coding. Meta RL.

Let's try to develop an intuition about how the dynamics of a neural network can compute an algorithm. We will consider the Principal Component Analysis (PCA) algorithm.

![Gaussian data]({{ site.baseurl }}/assets/img/nn_pca/data.png)

## PCA

PCA captures the directions of maximal variance of the data. The coordinates along these directions are denoted by $$z$$. The reconstruction error for a given data point $$x$$ is given by $$||Wz-x||^2$$. 
$$
\min_{\mathbf{z}} \frac{1}{2}||W\mathbf{z}-\mathbf{x}||^2 =: R_\mathbf{x}(\mathbf{z})
$$
The solution can be computed analytically using the SVD decomposition of $$X$$ 
$$
\nabla_\mathbf{z} R_\mathbf{x}(\mathbf{z}) = W^TW\mathbf{z} - W^T \mathbf{x} = 0
$$


The solution is $$z=(W^TW)^{-1}W^Tx$$ 



The weights $$W$$ minimizes the expectation of reconstruction error under the data generating distribution. 
$$
\min_{W} \frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_{data}}[||W\mathbf{z}-\mathbf{x}||^2] =: R_{\mathbf{z}}(W)
$$

$$
\dfrac{\partial R_{\mathbf{z}}}{\partial W_{ij}}(W) = (\hat{x}_k-x_k)z_j
$$

The matrix $$W$$ can also be computed using the SVD decomposition of the matrix $$X$$ containing the data points in its rows.

Assume that the data are centered. $$X = U\Sigma V$$ and the covariance matrix is $$\dfrac{1}{n}X^TX = \frac{1}{n}V\Sigma^2 V^T$$. Change of variable $$\mathbf{z} = V^T\mathbf{x}$$ such that the variables $$\mathbf{z}$$ are uncorrelated, i.e. diagonal covariance matrix.

Approximate it by a low rank matrix $$W$$
$$
\dfrac{1}{n}X^TX \approx \sigma_1^2 v_1v_1^T + ... + \sigma_r^2v_rv_r^T
$$
It gives the solution of 



## Dynamical Equations

Let's denote by $$\mathbf{\hat{x}}$$ the activity of the neurons computing $$W\mathbf{z}$$. Their dynamics is described by the following equation
$$
\tau \dfrac{d\mathbf{\hat{x}}}{dt} = -\mathbf{\hat{x}} + W\mathbf{z}
$$
Linear rate model.

The equilibrium solution for which $$\dfrac{d\mathbf{\hat{x}}}{dt}=0$$ is $$ \mathbf{\hat{x}} = W\mathbf{z}$$, and the rate of convergence towards this equilibrium is controlled by the time constant $\tau$. To obtain the dynamical equations of $\mathbf{z}$ and the weights $$W$$, 

Let's consider the system
$$
\dfrac{d\mathbf{x}}{dt} = - \nabla V(\mathbf{x})
$$
We have that $$\dfrac{dV}{dt}(\mathbf{x}(t)) = \nabla V(\mathbf{x}(t)) \cdot \dfrac{d\mathbf{x}}{dt}(t) = -||\nabla V(\mathbf{x}(t))||^2 \leq 0$$, and it is equal to $$0$$ if and only if $$\nabla V = 0$$. The function $$V$$ can only decrease on the trajectories of the system and the state of the system converges towards a minimum of $$V$$ (assuming $$V$$ is convex). This is the equivalent of the gradient descent algorithm but in continuous time.

Knowing this, we can construct dynamical equations from the objective function that we would like to minimize.




$$
\tau \dfrac{dz}{dt} = W^T(x-\hat{x})
$$

$$
\tau_W \dfrac{dW}{dt} = (x-\hat{x})z^T
$$

Two different timescales. 

The slow dynamics of the weights make them sensitive to the data distribution instead of the details of individual examples.

## Simulations

![Animation PCA on Gaussian data]({{ site.baseurl }}/assets/img/nn_pca/pca_anim.gif)



### TODO

- [x] PCA on MNIST (only 0 and 1 digits so that can be separated with PCA), animation with the projection on the two first principal components (show them in light orange and display the normalized column vectors of W projected onto that plane)
- [ ] Display the animation in the reference frame of the 2 first principal components and  in the reference frame represented by the learned weights of the network
- [ ] Add a temporal filtering to z
- [ ] Nonnegative PCA ?



