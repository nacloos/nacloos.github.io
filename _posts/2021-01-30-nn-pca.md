---
layout: post
title: Fast and slow dynamics performing PCA
tags: [neuronal dynamics]
---

The brain can be viewed as a computing or information processing machine where the relevant information is extracted from the sensory inputs. Similarly to our computers, the computations that the brain performs can be described with algorithms. However, the brain is also very different from our computers and others prefer to view it as a dynamical system with ongoing dynamics at multiple temporal and spatial scales. How can we combine these two points of view ?

In this blog post, we will try to gain intuition about how the dynamics of a neural network can compute an algorithm. As an example we will consider the Principal Component Analysis (PCA) method, which is a classical method for dimensionality reduction. The idea is to project high-dimensional data onto a smaller subspace in a way that captures as much as possible the relevant aspects of the data, and we will see what does that mean more precisely. This can be useful to visualize the data in a two-dimensional plane for example. After reviewing the basics of PCA, we will use a formulation of PCA as an optimization problem to build a dynamical system that implements this algorithm. In particular, we will see the importance of having two distinct time scales for the dynamics. Finally, we will see how it performs on Gaussian data and on images of digits.

<!--Two views on the brain. One is closer to artificial intelligence and sees the brain as a computing machine. Algorithms are a list of instructions that are executed one at a time.-->

<!--The other sees it as a dynamical systems and study neuronal dynamics. Dynamical systems are described by a differential equation giving the time evolution of the state of the system. So here the evolution is specified in continuous time instead of the discrete time of algorithms.-->



<!--Quite general framework that allows us to build a dynamical system whose state evolves to minimize some objective or energy function.-->



## PCA
Let's consider $$m$$ data points $$\mathbf{x}^{(1)}, ..., \mathbf{x}^{(m)}$$ in $$\mathbb{R}^n$$ drawn independently and identically (i.d.d.) from a distribution $$p_{data}$$, which is often called the data generating distribution. The PCA method assumes that the relevant directions are those along which the data vary the most, while those along which the data vary the least are considered as noise. It is important to note that this is an assumption and that it is not always satisfied. Depending on what we want to do with the data, the directions of small variance might contain meaningful information.

Let's consider a toy example in $$\mathbb{R}^2$$ so that we can fully visualize what is happening. On the figure below is shown some data points sampled from a Gaussian distribution. I have represented in solid line the direction along which the data vary the most and let's see how we can compute it.

<img src="{{ site.baseurl }}/assets/img/nn_pca/data.png" class="center">

> **Remark:** *The covariance of two random variables $$x_1$$ and $$x_2$$ is defined as $$\text{cov}(x_1, x_2) = \mathbb{E}[(x_1 - \mathbb{E}[x_1])(x_2 - \mathbb{E}[x_2])]$$, which is equal to $$\mathbb{E}[x_1x_2]$$ if the variables have been centered beforehand so that they have zero mean. The entry at the $$i^{\text{th}}$$ row, $$j^{\text{th}}$$ column of the covariance matrix of a vector of random variables $$\mathbf{x} = (x_1, ..., x_n)$$ is given by $$\text{cov}(x_i, x_j)$$. The expectation over a random variable can be approximated by an average over data points sampled i.d.d. from the distribution of the random variable. Thus, for centered data, $$\text{cov}(x_i, x_j) \approx \frac{1}{m}\sum_{k=1}^m x_i^{(k)}x_j^{(k)}$$ and the covariance matrix can be approximated by $$\frac{1}{m}X^TX$$.*




Assume that the data are centered. $$X = U\Sigma V$$ and the covariance matrix is $$\dfrac{1}{n}X^TX = \frac{1}{n}V\Sigma^2 V^T$$. Change of variable $$\mathbf{z} = V^T\mathbf{x}$$ such that the variables $$\mathbf{z}$$ are uncorrelated, i.e. diagonal covariance matrix.

Approximate it by a low rank matrix $$W$$

$$
\dfrac{1}{n}X^TX \approx \sigma_1^2 v_1v_1^T + ... + \sigma_r^2v_rv_r^T
$$



This is in fact the solution of an optimization problem.



PCA captures the directions of maximal variance of the data. The coordinates along these directions are denoted by $$z$$. The reconstruction error for a given data point $$x$$ is given by $$||Wz-x||^2$$. 
$$
\min_{\mathbf{z}} \frac{1}{2}||W\mathbf{z}-\mathbf{x}||^2 =: R_\mathbf{x}(\mathbf{z})
$$

The solution can be computed analytically using the SVD decomposition of $$X$$ 

$$
\nabla_\mathbf{z} R_\mathbf{x}(\mathbf{z}) = W^TW\mathbf{z} - W^T \mathbf{x} = 0
$$


The solution is $$z=(W^TW)^{-1}W^Tx$$ 



The weights $$W$$ minimizes the expectation of reconstruction error under the data generating distribution. 

$$
\min_{W} \frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_{data}}[||W\mathbf{z}-\mathbf{x}||^2] =: R_{\mathbf{z}}(W)
$$

$$
\dfrac{\partial R_{\mathbf{z}}}{\partial W_{ij}}(W) = (\hat{x}_k-x_k)z_j
$$

The matrix $$W$$ can also be computed using the SVD decomposition of the matrix $$X$$ containing the data points in its rows.





## Dynamical Equations

### Fast dynamics

Let's denote by $$\mathbf{\hat{x}}$$ the activity of the neurons computing $$W\mathbf{z}$$. Their dynamics is described by the following equation
$$
\tau \dfrac{d\mathbf{\hat{x}}}{dt} = -\mathbf{\hat{x}} + W\mathbf{z}
$$

Linear rate model.
The equilibrium solution for which $$\dfrac{d\mathbf{\hat{x}}}{dt}=0$$ is $$ \mathbf{\hat{x}} = W\mathbf{z}$$, and the rate of convergence towards this equilibrium is controlled by the time constant $$\tau$$. To obtain the dynamical equations of $$\mathbf{z}$$ and the weights $$W$$, 

> **Theorem:** *If a dynamical system can be written as $$ \frac{d\mathbf{x}}{dt} = - \nabla V(\mathbf{x})$$  then*
> 
> $$
>\frac{dV}{dt}(\mathbf{x}(t)) = \nabla V(\mathbf{x}(t)) \cdot \frac{d\mathbf{x}}{dt}(t) = -||\nabla V(\mathbf{x}(t))||^2 \leq 0
> $$
> 
> *and $$\frac{dV}{dt} = 0$$ if and only if $$\nabla V = 0$$. The function $$V$$ can only decrease on the trajectories of the system and the state of the system converges towards a minimum of $$V$$ (assuming $$V$$ is convex). This is the equivalent of the gradient descent algorithm but in continuous time.*




Knowing this, we can construct dynamical equations from the objective function that we would like to minimize.



$$
\tau \dfrac{dz}{dt} = W^T(x-\hat{x})
$$

### Slow dynamics

$$
\tau_W \dfrac{dW}{dt} = (x-\hat{x})z^T
$$

Two different timescales. 

The slow dynamics of the weights make them sensitive to the data distribution instead of the details of individual examples.

## Simulations

<img src="{{ site.baseurl }}/assets/img/nn_pca/pca_anim.gif" class="center">



## Conclusion

Rao, predictive coding. Meta RL.





### TODO

- [x] PCA on MNIST (only 0 and 1 digits so that can be separated with PCA), animation with the projection on the two first principal components (show them in light orange and display the normalized column vectors of W projected onto that plane)
- [ ] Display the animation in the reference frame of the 2 first principal components and  in the reference frame represented by the learned weights of the network
- [ ] Add a temporal filtering to z
- [ ] Nonnegative PCA ?



