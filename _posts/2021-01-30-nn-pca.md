---
layout: post
title: Fast and slow dynamics computing PCA
tags: [neuronal dynamics, machine learning]
---

The brain can be viewed as a computing or information processing machine where the relevant information is extracted from the sensory inputs. Similarly to our computers, the computations can be described as a sequence of instructions with algorithms. However, the brain is also very different from our computers and others prefer to view it as a dynamical system with continuous time dynamics at multiple temporal and spatial scales. How can we combine these two points of view ?

In this blog post, I will try to provide some intuition about how the dynamics of a neural network can compute an algorithm. As an example I will consider the Principal Component Analysis (PCA) method, which is a classical method for dimensionality reduction. The idea is to project high-dimensional data onto a smaller subspace in a way that captures as much as possible the relevant aspects of the data, and we will see what does that mean more precisely. After reviewing the basics of PCA, we will use a formulation of PCA as an optimization problem to build a continuous time dynamical system that computes this algorithm. In particular, we will see the importance of having two separate time scales for the dynamics. Finally, we will see how it performs on Gaussian data <!--and on the MNIST dataset-->.

<!--Two views on the brain. One is closer to artificial intelligence and sees the brain as a computing machine. Algorithms are a list of instructions that are executed one at a time.-->

<!--The other sees it as a dynamical systems and study neuronal dynamics. Dynamical systems are described by a differential equation giving the time evolution of the state of the system. So here the evolution is specified in continuous time instead of the discrete time of algorithms.-->



<!--Quite general framework that allows us to build a dynamical system whose state evolves to minimize some objective or energy function.-->



## Principal Component Analysis
Let's consider $$m$$ data points $$\mathbf{x}^{(1)}, ..., \mathbf{x}^{(m)}$$ in $$\mathbb{R}^n$$ drawn independently and identically (i.d.d.) from a distribution $$p_{data}$$, which is often called the data generating distribution. The PCA method assumes that the relevant directions are those along which the data vary the most, while those along which the data vary the least are considered as noise. It is important to note that this is an assumption and it will be satisfied or not depending on what your data represent and what you want to do with it. We can visualize more concretely what is a direction of maximal variance with a small example in $$\mathbb{R}^2$$. On the figure below is shown some data points sampled from a Gaussian distribution and we can see that there is a direction along which the data vary the most, depicted by the solid line segment. 

<img src="{{ site.baseurl }}/assets/img/nn_pca/data.png" class="center">

The data $$\mathbf{x}^{(1)}, ..., \mathbf{x}^{(m)}$$ are assumed to be centered and let's place them in a matrix $$X$$ such that the $$i^{\text{th}}$$ row is given by $$\mathbf{x}^{(i)}$$. The goal of PCA is to find a subspace of dimension $$r$$ that captures as much variance as possible and to find an uncorrelated basis of this subspace. We will need the notion of the covariance matrix, so here is a short remark for those who are not familiar with it.

> **Remark:** *The covariance of two random variables $$x_1$$ and $$x_2$$ is defined as $$\text{cov}(x_1, x_2) = \mathbb{E}[(x_1 - \mathbb{E}[x_1])(x_2 - \mathbb{E}[x_2])]$$, which is equal to $$\mathbb{E}[x_1x_2]$$ if the variables have been centered beforehand so that they have zero mean. The entry at the $$i^{\text{th}}$$ row, $$j^{\text{th}}$$ column of the covariance matrix of a vector of random variables $$\mathbf{x} = (x_1, ..., x_n)$$ is given by $$\text{cov}(x_i, x_j)$$. The expectation over a random variable can be approximated by an average over data points sampled i.d.d. from its distribution	. Thus, for centered data, $$\text{cov}(x_i, x_j) \approx \frac{1}{m}\sum_{k=1}^m x_i^{(k)}x_j^{(k)}$$ and the covariance matrix can be approximated by $$\frac{1}{m}X^TX$$.  From now on, I will use this approximation but won't make explicitly the distinction between random variable and data sample.*

Let's start by computing the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) $$X = U\Sigma V^T$$ where $$U$$ and $$V$$ are orthogonal matrices, and use it to define the following change of variable

$$
\mathbf{y} = V^T\mathbf{x}
$$

This change of variable can be applied on each example $$\mathbf{x}^{(i)}$$ in $$X$$ to obtain a matrix $$Y=XV$$. Then using the fact that $$X^TX = V\Sigma^2 V^T$$, the covariance matrix of $$Y$$ is given by

$$
\dfrac{1}{n}Y^TY = \frac{1}{n}\Sigma^2
$$

Since $$\Sigma$$ is diagonal, the covariance matrix of $$\mathbf{y}$$ is diagonal. So we have $$n$$ uncorrelated random variables $$y_i$$ with respective variance given by the diagonal elements of $$\Sigma^2$$, i.e. $$\text{var}(y_i) = \sigma_i^2$$. From a geometrical point of view, the variable $$y_i$$ gives the component of the vector $$\mathbf{x}$$ projected onto the basis vector $$\mathbf{v}_i$$ which is the $$i^{\text{th}}$$ column of $$V$$. Hence, the direction of maximal variance is given by the vector $$\mathbf{v}_1$$ associated with the maximal singular value $$\sigma_1$$ (the singular values are ordered in decreasing order) and the corresponding variance is $$\text{var}(y_1) = \sigma_1^2$$. More generally, the subspace spanned by the $$r$$ first basis vectors $$\mathbf{v}_1, ..., \mathbf{v}_r$$ associated with the $$r$$ largest singular values $$\sigma_1, ..., \sigma_r$$ is the subspace of dimension  $$r$$ that captures as much variance of the data as possible.

<!--Approximate it by a low rank matrix $$W$$-->
<!--$$
\dfrac{1}{n}X^TX \approx \sigma_1^2 v_1v_1^T + ... + \sigma_r^2v_rv_r^T
$$-->



### Optimization formulation

To build a dynamical system that computes the solution obtained above, it will be useful to express this solution as the solution of an optimization problem. Above we defined the vector $$\mathbf{y}\in\mathbb{R}^n$$ as $$\mathbf{y}=V^T\mathbf{x}$$, which contains the components of the orthogonal projection of the vector $$\mathbf{x}$$ onto the columns of $$V$$. As the goal is to compute the $$r$$-dimensional subspace of maximal variance, we are just interested in computing for each data point $$\mathbf{x}$$,


$$
\mathbf{z} = W^T \mathbf{x}
$$


where $$W \in \mathbb{R}^{n\times r}$$ is the matrix containing the $$r$$ first columns of $$V$$, and $$ \mathbf{z} \in \mathbb{R}^r$$ contains the components  of the projection of $$ \mathbf{x}$$ onto the columns of $$W$$. Now let's show that this definition of $$ \mathbf{z}$$ and $$W$$ is the solution of an optimization problem. To this end, we will use the following theorem.



<!--and let's consider the projection of $$\mathbf{x}$$ onto the subspace spanned by the columns of $$W$$. If we relax the need to have an orthonormal basis of this subspace, the components $$\mathbf{z}\in\mathbb{R}^r$$ of the projection of $$\mathbf{x}$$ onto the columns of $$W$$ is given by

$$
\mathbf{z}=(W^TW)^{-1}W^T\mathbf{x}
$$

which is a well-known formula for the [orthogonal projection](https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Formulas) on non orthonormal vectors. The projected vector, denoted $$\mathbf{\hat{x}}$$, is then obtained by taking linear combinations of the columns of $$W$$, i.e.  $$\mathbf{\hat{x}} = W\mathbf{z}$$. Now observe that $$\mathbf{z}$$ is the solution of the following optimization problem

$$
\min_{\mathbf{z}} \frac{1}{2}||W\mathbf{z}-\mathbf{x}||^2
$$

To see this, you can take the derivative of the objective and set it to zero. More intuitively, what we are trying to find the linear combination of are the coefficients of the linear combination of the so that the error between the reconstructed vector $$\mathbf{\hat{x}}$$ and the data vector $$\mathbf{x}$$ is minimal.-->







<!--The weights $$W$$ minimizes the expectation of reconstruction error under the data generating distribution. 

$$
\min_{W} \frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_{data}}[||W\mathbf{z}-\mathbf{x}||^2] =: R_{\mathbf{z}}(W)
$$

$$
\dfrac{\partial R_{\mathbf{z}}}{\partial W_{ij}}(W) = (\hat{x}_k-x_k)z_j
$$


The matrix $$W$$ can also be computed using the SVD decomposition of the matrix $$X$$ containing the data points in its rows.-->





> **Theorem ([Low-rank approximation](https://en.wikipedia.org/wiki/Low-rank_approximation#Basic_low-rank_approximation_problem)):** The solution of 
> 
> $$
> \min_{\hat{X}} ||\hat{X} - X||^2_F \quad \text{s.t. } \ \text{rank}(\hat{X}) \leq r
> $$
> 
> is $$\hat{X} = \sigma_1 \mathbf{u}_1\mathbf{v}_1^T + ... + \sigma_r \mathbf{u}_r\mathbf{v}_r^T$$, where $$X = U\Sigma V^T$$ is the SVD decomposition of $$X$$.



Let's start with arbitrary $$\mathbf{z}^{(i)}$$ and matrix $$W$$ and let's consider the minimization of the average reconstruction error between a data point $$\mathbf{x}^{(i)}$$ and the vector $$W \mathbf{z}^{(i)}$$,

$$
\min_{\mathbf{z}^{(i)}, W} \ \frac{1}{m}\sum_{i=1}^m||W\mathbf{z}^{(i)}-\mathbf{x}^{(i)}||_2^2 = \frac{1}{m} ||\hat{X}-X||^2_F
$$

where $$\vert\vert\cdot\vert\vert_F$$ is the Frobenius norm of a matrix, which can be written as the sum of the squared Euclidian norms of the rows of the matrix. The matrix $$\hat{X}$$ contains the vectors $$\mathbf{\hat{x}}^{(i)} = W \mathbf{z}^{(i)}$$, $$i = 1, ..., m$$, in its rows and its rank is indeed lower or equal to $$r$$. Using the low-rank approximation theorem, the solution is


$$
\begin{aligned}
\mathbf{\hat{x}}^{(i)} &\equiv z_1^{(i)}\mathbf{w}_1 + ... + z_r^{(i)}\mathbf{w}_r \\
&= \sigma_1 U_{i1} \mathbf{v}_1 + ... + \sigma_r U_{ir} \mathbf{v}_r
\end{aligned}
$$


where $$\equiv$$ means that the equality holds by definition. From the fact that $$W$$ is constant for all data points and that it is $$\mathbf{z}^{(i)}$$ which depends on the input data $$\mathbf{x}^{(i)}$$, we can identify $$z^{(i)}_j$$ with $$ \sigma_j U_{ij}$$  and the columns of $$W$$ with the vectors $$\mathbf{v}_1, ..., \mathbf{v}_r$$. Thus, we have obtained what we wanted for $$W$$ and actually for $$\mathbf{z}$$ too since

$$
W^T \mathbf{x}^{(i)} = W^T (U_{i,:}\Sigma V^T)^T = \begin{bmatrix}\sigma_1U_{i1} \\ \vdots \\ \sigma_rU_{ir}\end{bmatrix} \equiv \mathbf{z}^{(i)}
$$

If we combine this result with the intuition from the previous section, projecting data points on the subspace of dimension $$r$$ which captures as much variance of the data as possible minimizes the average Euclidean norm of the reconstruction error. In this sense $$\mathbf{z}$$ is the best linear representation of dimension $$r$$ of the data. This is particularly useful when the data is high-dimensional since with $$r \ll n$$, we have a low-dimensional representation that can be useful to visualize the data for example.



## Dynamical Equations

Now it's time to derive the dynamical equations of a neural network computing PCA. We will need 3 groups of neurons: one for the input data $$\mathbf{x}$$, one for the latent variables $$\mathbf{z}$$, and one for the reconstructed data $$\mathbf{\hat{x}}$$. The connections between those neurons have weights given by the matrix $$W$$.

The dynamics of the neurons and the dynamics of the weights are governed by the same objective function, the difference resides in the their time scale.



Need to specify the dynamics of the external input of the neural network. Suppose that a data point is drawn randomly from the data set and that it is presented to the neural network for a fixed amount of time.

### <!--Fast dynamics-->

First, we need some units that compute the orthogonal projection of the input vector onto 





Let's denote by $$\mathbf{\hat{x}}$$ the activity of the neurons computing $$W\mathbf{z}$$. A simple dynamical model that has $$\mathbf{\hat{x}} = W\mathbf{z}$$ as its equilibrium solution is given by

$$
\tau \dfrac{d\mathbf{\hat{x}}}{dt} = -\mathbf{\hat{x}} + W\mathbf{z}
$$

which is called a rate model in computational neuroscience. The time constant $$\tau$$ controls the rate of convergence towards the equilibrium.

To obtain a dynamical equation for $$\mathbf{z}$$ and $$W$$, we will use the optimization problem of the previous section together with the following theorem. 

> **Theorem:** *If a dynamical system can be written as $$ \frac{d\mathbf{x}}{dt} = - \nabla V(\mathbf{x})$$  then*
> $$
> \frac{dV}{dt}(\mathbf{x}(t)) = \nabla V(\mathbf{x}(t)) \cdot \frac{d\mathbf{x}}{dt}(t) = -||\nabla V(\mathbf{x}(t))||^2 \leq 0
>$$
> 
> *and $$\frac{dV}{dt} = 0$$ if and only if $$\nabla V = 0$$. The function $$V$$ can only decrease on the trajectories of the system and the state of the system converges towards a minimum of $$V$$ (assuming $$V$$ is convex). This is the equivalent of the gradient descent algorithm but in continuous time.*

So if we take $$V$$ to be the objective that we are trying to minimize to compute the solution of PCA, its derivatives give us dynamical equations for $$\mathbf{z}$$ and $$W$$.



The time scale of the dynamics of $$\mathbf{z}$$ is much smaller than the amount of time that a data point $$\mathbf{x}$$ is presented.

$$
\tau \dfrac{dz}{dt} = W^T(x-\hat{x})
$$
### <!--Slow dynamics-->

The expectation over the data is replaced by a time average and slow dynamics. Equivalent of stochastic gradient descent where the expectation of the gradient is replaced by the gradient evaluated for one data point sampled randomly.
$$
\tau_W \dfrac{dW}{dt} = (x-\hat{x})z^T
$$

<!--Two--> different timescales. 

The slow dynamics of the weights make them sensitive to the data distribution instead of the details of individual examples.

Hebbian learning.

The weights capture the statistical regularities, the patterns in the activity of the neurons.



To recapitulate,


$$
\left\{
    \begin{array}{l}
        \begin{aligned}
\tau_{fast} \ \dot{\mathbf{\hat{x}}} &= -\mathbf{\hat{x}} + W\mathbf{z}\\
\tau_{fast} \ \dot{\mathbf{z}} &= W^T(\mathbf{x}-\mathbf{\hat{x}})\\
\tau_{slow} \ \dot{W} &= (\mathbf{x}-\mathbf{\hat{x}})\mathbf{z}^T
\end{aligned}
    \end{array}
\right.
$$


and $$\mathbf{x}$$ is sampled randomly from the data set at intervals $$\tau$$, with $$\tau_{fast} \ll \tau \ll \tau_{slow}$$.

## Simulations

<img src="{{ site.baseurl }}/assets/img/nn_pca/pca_anim.gif" class="center">



## Conclusion

Rao, predictive coding. Meta RL.

Fast dynamics called perception and slow dynamics called learning. Both perception and learning minimize the same objective.



<!--Do it on natural images with nonnegative pca and observe gabor receptive field ?-->



<!--Material: MOOC Coursera-->

### <!--TODO-->

- [x] <!--PCA on MNIST (only 0 and 1 digits so that can be separated with PCA), animation with the projection on the two first principal components (show them in light orange and display the normalized column vectors of W projected onto that plane)-->
- [ ] <!--Display the animation in the reference frame of the 2 first principal components and  in the reference frame represented by the learned weights of the network-->
- [ ] <!--Add a temporal filtering to z-->
- [ ] <!--Nonnegative PCA ?-->



